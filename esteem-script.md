# On Esteem: Rewarding Reproducible Science

Andy Wills

## SLIDE 1: Introduction

Esteem is crucial in science -- it determines who gets a job, who gets
promoted, and who gets access to research funds through grants. For all these
reasons, it's essential that our measures of esteem are at least compatible
with doing good reproducible science and, ideally, reward it. 

## SLIDE 2: Norms of esteem

I'm going to focus this talk on hiring and promotion decisions. So, if you don't make those decisions, should you stop listening now? Of course not! Promotion and hiring decisions don't exist in a vacuum, they largely reflect the norms of our group. And we all collectively contribute to those norms. Every time you:

- Introduce a visiting speaker
- Write a self-bio
- Advise or decide on which journal to submit to
- Congratulate a colleague on an academic success
- Criticise or gossip about a colleague's academic shortcomings
- Decide which papers to read, or to cite.

You're feeding into these norms. So now I'm gonna talk about some good, and not so good, ways to measure esteem in research. I'm trying to keep this pretty short, so I'm going to focus on publications. There's also a fair amount one could say about other potential esteem indicators, like grant awards and wider impact, but that's probably another talk.

Publications are of course the reports of the science we've done, so esteem measures based on publication should in principle be highly amenable to rewarding good reproducible science. But I don't think we do this very well at the moment.

### SLIDE 3: Number of publications

The total number of papers you've published is clearly an esteem indicator we currently use. We introduce visiting speakers by praising the number of papers they've published. We have hiring decisions that very explicitly incude number of publications as a measure of esteem (one paper per year since the beginning of your Ph.D. for a prestiguous post-doc position; 50 papers for a Chair). We admire colleagues for prolific publication rates, and look down on people who publish below a certain rate. 

We really need to stop doing all that kind of shit. For a start...

### SLIDE 4: Issues with N

...it's incredibly easy to game -- just make some reciprocal co-author
arrangements to get on each other's papers for minimal input. And, in any case,
getting work published in peer-reviewed journals, is mainly a measure of
persistence and/or wealth. If you (or your university) is rich, you can pay to
send your article to peer-reviewed journals that basically never reject a paper
(like Frontiers). If you're poor, you just keep sending stuff to different
journals until you find one where they reviewers like what you do and or have
spent some time drinking heavily at conferences with you.

### SLIDE 5: Location of publications

We also seem to make a lot of use of Impact Factors. Think about conversations you've had about where to send your latest manuscript. Chances are they involved impact factor. 

Or if you've reviewed CVs for a hiring panel, how often have you just looked at where the article was published rather than, say, seeing if anyone had cited it or, heaven forbid, actually reading the damm thing.

Impact Factors were designed to help libraries decide which journals to subscribe to back when things like physical shelf space were actually an issue. They were never intended as a measure of the quality of individual papers within those journals, and they work incredibly badly as a measure of individual paper esteem.

The impact factor of a journal is the mean number of citations papers get in the first two years of publication. 

### SLIDE 6: Impact factor is unrepresentative

The single most important thing to understand about impact factors is that the distribution is massively skewed, with a very small number of papers being cited a very large number of times, while most papers -- irrespective of journal -- don't get cited much in the first two years.

As any first year psychology student knows, means are poor summaries of skewed distributions. Accordingly, the impact factor of a journal is unrepresentative of most papers it contains. It's largely driven by the journal's ability to attract super-cited papers occasionally. So, it tells you very little about a specific paper, and so shouldn't be used as a measure of esteem of an individual's work. 

Put this another way. If a paper is very highly cited, the chances it's in a high IF journal are high (because that paper contributed substantially to the journal's IF). But the reverse is not true. If a paper is in a high IF journal, the chances are it's *not* particularly well cited ... because most papers aren't. 

### SLIDE 7: Impact factor is uncorrelated with quality

Also, the correlation between IF and measures of reproducibility are non-existent or negative. For example, statistical power either remains stable or drops as IF goes up.

### SLIDE 8: Number of citations

Citation data is better, in the sense that it is at least about the actual
paper, rather than the journal it appears in. 

There is a potential concern about 'shit-tations' i.e. work being highly cited
because many many people feel obliged to say in print that the work is no
good. Analyses suggest shit-tations are a small proprotion of total citations
(perhaps 3%), although my experience is that those are not evenly distributed across researchers, so for hiring and promotion decisions, we should not rely on citations alone.

Other issues around citations include that citation practice differs across different parts of psychology, and is affected by the number of people working on that problem. So, while I think we can probaby say that a paper getting 10 citations in 5 years is a measure of esteem relative to getting 0 citations in 5 years, whether getting 100 citations rather than 10 is twice as good, ten times as good, or about the same is really hard to judge.

One way of evening these things out is to use h-index. Your h-index is the number of papers you've had cited at least that number of times. So, if your h-index is 5, you have 5 papers that have each been cited at least 5 times. For 6, you need 6 papers cited at least 6 times, and so on. 

As a rule of thumb, h-index rises by 1 each year for the averagely research active full-time academic at a research-intensive institution. You can even get distributional information for different job grades in Psychology (link provided).

So, if you were going to try to measure esteem without reading the work, you're
probably best to use h-index. Similarly, if you want to make esteem-related comments about visiting speakers, or your colleagues, without having read their work, then h-index would be the thing to focus on.

But there's a couple of major caveats here:

1. h-index can still be pretty easily gamed through the 'co-author'
system. Just find some really influential researchers,fill some minor niche for
them, and get your name on the paper. In psychology, anything vaguely technical
or numerical generally does the trick. Or access to patients or kit.

2. h-index is basically a measure of sustained contribution to a field. So it's really no good for junior hires. It probably is OK for mid-career (i.e. AP upwards) appointments. It's a bit of a myth that 20th century geniuses only published a handful of papers. For example, Einstein published his first paper on special relativity in 1905, but he'd published about 20 papers before that.

### SLIDE 9: Read the damm thing!

Another approach we could take here is to not pass opinion about how good someone's research is unless you've read it -- or at least the abstract plus enough of the rest of it to make an informed judgment. 

So, when looking for something to say about an invited speaker, perhaps ask them ahead of time what their best three papers are, read them if you haven't already, and find something nice to say about one of them. 

"Jackie's 2005 paper on the elaborated Intrusions theory of desire made me think completely differently about how to reduce obesity"

If you ever find yourself able to influence an appointments or promotion panel, think about how to include an assessment of esteem that involves someone actually having read something!

For example, require candidates to list what they consider to be their three best publications. Read them and talk to them about at least one of them at interview. Perhaps get in some external expert opinion, although make sure it's not a mate or co-author of the candidate (for this reason, probably don't ask the candidate or anyone with a vested interest in that person being appointed).

This sounds like a lot of work. It is. It's worth it. Appointing someone to a permanent lectureship at 30 is committing to over Â£1m in salary costs if they stay for their whole career. Perhaps consider distributing the reading across all senior staff at a first pass, and the panel reading only after some shortlisting. 

There are a couple of potential problems with this approach

1. Author contribution: 

How much was this great paper actually their work? Very hard to tell unless it's a sole-author manuscript, and those are increasingly rare. Encourgaing the norm of including author contribution statements in papers can help a bit. So, probably, can talking to the person at interview about the paper. 

2. Hypercompetition

Say you get 100 applicants for a lecturer position - you really gonna read 300 papers, even spread across 20 staff members? That may not be feasible, and so you might have to use h-index, citations, or even N of papers. But look at this as a threshold only - say as one of several factors you use to get from 100 applicants to 20. 

The other, harder, problem is to address hypercompetition. Some options here include:

- As a field, supervise fewer Ph.D. students, and encourage Ph.D. students to consider academia as _one_ destination, not the only one, or even necessarily the best one. Seems to me like a really good way to have impact is if our best trained students - the ones with PhDs - go out and interact with the rest of the world. 

- Have an appointments strategy: For example, advertise within a particular sub-discipline that the School is already known for. Or go out with the explicit intention of building an entirely new group, with linked appointments. 

## SLIDE 10: Grants

In the little time I've got left, let's talk about grants as a measure of esteem.

Overall, using grants to measure esteem is pretty much a terrible idea. It encourages people to apply for public money for reasons other than it being needed to do their research. It encourages them to prioritise what will get funded over what is important. That is at best disingenous and at worst a serious misuse of public funds. 

Grants are (sometimes) what you need to get good science done. If you do good science, that will eventually be represented in publications, and we can use those to measure esteem. If you're doing great work without big grants surely that doesn't make the work less great? And if you're getting big bucks but not coming up with the goods, surely that's something we should be critical of, not reward?

This is, of course, different to saying grants are unimportant. They are often important, but for financial reasons. For example, if you're making a hiring or promotion decision, it's probably pretty relevant how much that person is going to cost you relative to how much income they generate. 

It's probably worth remembering that, at a 20:1 staff student ratio, there is Â£180k of fee income coming to an English university for each academic it employs at perhaps Â£60k inc. oncosts. Nonetheless, it's entirely understandable if you have a job candidate with a track record of getting 20% of their time paid for by a research council that this is something that might affect a hiring decision.

So grants are important, and they affect hiring decisions and promotion decisions because they effectively reduce the cost of employment for the university. 

But let's not confuse that with esteem. Getting lots of money to do shoddy but really expensive research is not something we should celebrate. And great research that's cheap is still great, despite being cheap.

Try and reflect this in the esteem norms you use day to day. People who get some of their salary paid for through a grant deserve our thanks for helping to keep the university solvent. So does anyone else who helps protect or bolster the bottom line - through teaching, through consultancy, through spinoffs, through research, through adroit management.

But note that all of this is basically a matter of internal finances. Why
should anyone other than Oxford care when Prof. Plum at Oxford gets part of his
salary paid by the ESRC for the next three years? 

So, for example, when introducing an external speaker, probably don't say something like "has received over Â£2m in grant funding" -- because the only reason people at Plymouth should care about that (unless we're thinking of hiring them) is if grant income were a good measure of research quality. It isn't.

### SLIDE 11: Summary

So, in summary...we can all help each other do better, more reproducible science but helping to align our measures of esteem with things that are at least compatible with doing good science. To that end, try to quit using number of publications, impact factors, and grant income as measures of individual esteem. Depending on the context, it may be OK to use citations counts, or preferably the h-index. But the best thing is to ask people what their best work is...and read it.

### What are mine?

Wills, A.J., Inkster, A.B., & Milton, F. (2015). Combination or Differentiation? Two theories of processing order in classification

Wills, A.J., & Pothos, E.M. (2012). On the adequacy of current empirical evaluations of formal models of categorization.

Wills, A.J., Lavric, A., Croft, G., & Hodgson, T.L. (2007). Predictive learning, prediction errors and attention: Evidence from event-related potentials and eye tracking



---
Edmunds, C.E.R., Milton, F., & Wills, A.J. (2018). Due process in dual process

Wills, A.J., O'Connell, G., Edmunds, C.E.R., & Inkster, A.B. (2017). Progress in modeling through distributed collaboration:

Wills, A.J., Lavric, A., Hemmings, Y., & Surrey, E. (2014). Attention, predictive learning, and the inverse base-rate effect:

Wills, A.J., Reimers, S., Stewart, N., & McLaren, I.P.L. (2000). Tests of the ratio rule in categorization.



